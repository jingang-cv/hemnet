{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03bc65fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 80 from C header, got 96 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '6'\n",
    "\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "from scipy import io as sio\n",
    "\n",
    "from scipy import signal\n",
    "from torch.autograd import Variable\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from archs.cross_baseline_cat import BioPhysNet\n",
    "from datasets.rppg_datasets_reconstruction import COHFACE, PURE, UBFC, VIPL, MMSEHR\n",
    "from losses.NPLoss import Neg_Pearson\n",
    "from losses.CELoss import TorchLossComputer\n",
    "from utils.utils import AvgrageMeter, cxcorr_align, pearson_correlation_coefficient\n",
    "import math\n",
    "\n",
    "\n",
    "def cal_psd_hr(output, Fs, return_type):\n",
    "    \n",
    "    cur_device = output.device\n",
    "    \n",
    "    def compute_complex_absolute_given_k(output, k, N):\n",
    "        two_pi_n_over_N = 2 * math.pi * torch.arange(0, N, dtype=torch.float) / N\n",
    "        hanning = torch.from_numpy(np.hanning(N)).type(torch.FloatTensor).view(1, -1)\n",
    "\n",
    "        k = k.type(torch.FloatTensor).to(cur_device)\n",
    "        two_pi_n_over_N = two_pi_n_over_N.to(cur_device)\n",
    "        hanning = hanning.to(cur_device)\n",
    "        \n",
    "        output = output.view(1, -1) * hanning\n",
    "        output = output.view(1, 1, -1).type(torch.cuda.FloatTensor)\n",
    "        k = k.view(1, -1, 1)\n",
    "        two_pi_n_over_N = two_pi_n_over_N.view(1, 1, -1)\n",
    "        complex_absolute = torch.sum(output * torch.sin(k * two_pi_n_over_N), dim=-1) ** 2 \\\n",
    "                           + torch.sum(output * torch.cos(k * two_pi_n_over_N), dim=-1) ** 2\n",
    "        return complex_absolute\n",
    "    \n",
    "    output = output.view(1, -1)\n",
    "    \n",
    "    N = output.size()[1]\n",
    "    bpm_range = torch.arange(40, 180, dtype=torch.float)\n",
    "    unit_per_hz = Fs / N\n",
    "    feasible_bpm = bpm_range / 60.0\n",
    "    k = feasible_bpm / unit_per_hz\n",
    "    \n",
    "    # only calculate feasible PSD range [0.7,4]Hz\n",
    "    complex_absolute = compute_complex_absolute_given_k(output, k, N)\n",
    "    complex_absolute = (1.0 / complex_absolute.sum()) * complex_absolute\n",
    "    complex_absolute = complex_absolute.view(-1)\n",
    "    whole_max_val, whole_max_idx = complex_absolute.max(0) # max返回（values, indices）\n",
    "    whole_max_idx = whole_max_idx.type(torch.float) # 功率谱密度的峰值对应频率即为心率\n",
    "\n",
    "    if return_type == 'psd':\n",
    "        return complex_absolute\n",
    "    elif return_type == 'hr':\n",
    "        return whole_max_idx + 40\t# Analogous Softmax operator\n",
    "\n",
    "\n",
    "def set_seed(seed=92):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def _init_fn(seed=92):\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seed()\n",
    "\n",
    "frame_rate = 30\n",
    "bpm_range = torch.arange(40, 180, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecf3669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "## general parameters\n",
    "parser.add_argument('--num_rppg', type=int, default=160, help='the number of rPPG')\n",
    "parser.add_argument('--drop_rate', type=int, default=0.2, help='the drop rate of CodeResNet')\n",
    "parser.add_argument('--train_model', type=str, default='hemnet', help='train_model = [hemnet]')\n",
    "parser.add_argument('--dataset', type=str, default='PURE')\n",
    "### add for codephys\n",
    "parser.add_argument('--batch_size', type=int, default=2)\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--gamma', type=float, default=0.5)\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-5)\n",
    "parser.add_argument('--step_size', type=int, default=50)\n",
    "parser.add_argument('--eval_step', type=int, default=1)\n",
    "parser.add_argument('--epochs', type=int, default=300)\n",
    "parser.add_argument('--echo_batches', type=int, default=400, help='the number of mini-batches to print the loss')\n",
    "parser.add_argument('--save_path', type=str, default=\"/data/chushuyang/hemnet_exp_results\", help='the path to save the model [ckpt, code, visulization]')\n",
    "\n",
    "parser.add_argument('--align', type=bool, default=False)\n",
    "parser.add_argument('--proir_weight', default=1e-4)\n",
    "parser.add_argument('--appearance_weight', default=1e-3)\n",
    "parser.add_argument('--shading_weight', default=1e-3)\n",
    "parser.add_argument('--sparsity_weight', default=1e-7)\n",
    "\n",
    "parser.add_argument('--np_loss', default=True)\n",
    "parser.add_argument('--np_weight', default=0.01)\n",
    "parser.add_argument('--cn_loss', default=True)\n",
    "parser.add_argument('--cn_weight', default=1.0)\n",
    "parser.add_argument('--rec_loss', default=True)\n",
    "\n",
    "### model parameters\n",
    "parser.add_argument('--input_dim', type=int, default=3, help='the number of input channels')\n",
    "\n",
    "### datasets\n",
    "parser.add_argument('--norm_type', type=str, default='reconstruct')\n",
    "parser.add_argument('--aug', type=str, default='') # figsc\n",
    "parser.add_argument('--w', type=int, default=64, help='')\n",
    "parser.add_argument('--K', type=int, default=1, help=\"fold\")\n",
    "\n",
    "# args = parser.parse_args([\"--h\", \"128\", \"--dataset\", \"BUAA\"])\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff38f55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_80399/322852425.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  rppg_estimator.load_state_dict(torch.load(f'{ckpt_dir}/video_feature_extractor_{epoch_load}.pth', map_location=torch.device(device)))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型导入\n",
    "device = torch.device(f'cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "rppg_estimator = BioPhysNet(frames=args.num_rppg, device=device).to(device)\n",
    "ckpt_dir = '/data/chushuyang/hemnet_exp_results/1226_1413/ckpt' \n",
    "epoch_load = 'best'\n",
    "rppg_estimator.load_state_dict(torch.load(f'{ckpt_dir}/video_feature_extractor_{epoch_load}.pth', map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab8124c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2031])\n"
     ]
    }
   ],
   "source": [
    "# 数据集导入\n",
    "dataloader_test = DataLoader(PURE(train=True, T=-1, norm_type=args.norm_type, aug=\"\",w=args.w, h=args.w))\n",
    "print(next(iter(dataloader_test))['ecg'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e0c71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# video-level -- test\n",
    "from copy import deepcopy\n",
    "\n",
    "\n",
    "bpm_range = torch.arange(40, 180, dtype=torch.float).cuda()\n",
    "hr_gt = []\n",
    "hr_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sample_batched in tqdm(dataloader_test):\n",
    "        # get the inputs\n",
    "        inputs, ecg, clip_average_HR = sample_batched['video'].to(device),\\\n",
    "            sample_batched['ecg'].to(device), sample_batched['clip_avg_hr'].to(device)\n",
    "        if \"seg\" in sample_batched.keys():\n",
    "            mask = sample_batched['seg'].to(device)\n",
    "\n",
    "        num_clip = 3\n",
    "        input_len = inputs.shape[2]\n",
    "        input_len = input_len - input_len % (num_clip * 32)\n",
    "        clip_len = input_len // num_clip\n",
    "        inputs = inputs[:, :, :input_len, :, :]\n",
    "                        \n",
    "        ecg = ecg[:, :input_len]\n",
    "\n",
    "        new_args = deepcopy(args)\n",
    "        new_args.num_rppg = clip_len\n",
    "        val_rppg_estimator = BioPhysNet(frames=new_args.num_rppg, device=device).to(device)\n",
    "        val_rppg_estimator.load_state_dict(torch.load(f'{ckpt_dir}/video_feature_extractor_{epoch_load}.pth', weights_only=True))\n",
    "        val_rppg_estimator.eval()\n",
    "        psd_gt_total = 0\n",
    "        psd_pred_total = 0\n",
    "        for idx in range(num_clip):\n",
    "\n",
    "            inputs_iter = inputs[:, :, idx*clip_len : (idx+1)*clip_len, :, :]\n",
    "            ecg_iter = ecg[:, idx*clip_len : (idx+1)*clip_len]\n",
    "            \n",
    "\n",
    "            psd_gt = cal_psd_hr(ecg_iter, 30, return_type='psd')\n",
    "            psd_gt_total += psd_gt.view(-1).max(0)[1].cpu() + 40\n",
    "\n",
    "            ## for rppg_estimator:\n",
    "            all_inputs = {\n",
    "                'video': inputs_iter,\n",
    "            }\n",
    "            \n",
    "            if \"seg\" in sample_batched.keys():\n",
    "                mask_clip = mask[:, idx*clip_len : (idx+1)*clip_len, :, :].to(device)\n",
    "                all_inputs[\"seg\"] = mask_clip\n",
    "                \n",
    "            outputs = val_rppg_estimator(all_inputs)\n",
    "            rPPG = outputs['rppg']\n",
    "\n",
    "            psd_pred = cal_psd_hr(rPPG[0], 30, return_type='psd')\n",
    "            psd_pred_total += psd_pred.view(-1).max(0)[1].cpu() + 40\n",
    "\n",
    "        hr_pred.append(psd_pred_total / num_clip)\n",
    "        hr_gt.append(psd_gt_total / num_clip)\n",
    "\n",
    "\n",
    "print(f'mae of model: {np.mean(np.abs(np.array(hr_gt) - np.array(hr_pred)))}')\n",
    "print(f'rmse of model: {np.sqrt(np.mean(np.square(np.array(hr_gt) - np.array(hr_pred))))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rppg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
